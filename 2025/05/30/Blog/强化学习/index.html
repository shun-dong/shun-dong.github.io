

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/only.ico">
  <link rel="icon" href="/img/only.ico">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="采薇">
  <meta name="keywords" content="">
  
    <meta name="description" content="基本概念Eg. Grid statestate space Sactionaction space A(s)State transition  forbidden areaTabular representation: deterministic 因为事实上是三阶张量State transition probability: all situationEg. p(s_2|s_1,a_2)  Pol">
<meta property="og:type" content="article">
<meta property="og:title" content="强化学习">
<meta property="og:url" content="http://example.com/2025/05/30/Blog/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/index.html">
<meta property="og:site_name" content="采薇的博客">
<meta property="og:description" content="基本概念Eg. Grid statestate space Sactionaction space A(s)State transition  forbidden areaTabular representation: deterministic 因为事实上是三阶张量State transition probability: all situationEg. p(s_2|s_1,a_2)  Pol">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/1741338531957_d.png">
<meta property="og:image" content="http://example.com/IMG_0797.jpeg">
<meta property="og:image" content="http://example.com/Pasted_image_20250305200350.png">
<meta property="og:image" content="http://example.com/Pasted_image_20250305203222.png">
<meta property="og:image" content="http://example.com/Pasted_image_20250305221218.png">
<meta property="og:image" content="http://example.com/Pasted_image_20250305221357.png">
<meta property="og:image" content="http://example.com/Pasted_image_20250305223543.png">
<meta property="og:image" content="http://example.com/1741332389063_d.png">
<meta property="og:image" content="http://example.com/Pasted_image_20250305230829.png">
<meta property="og:image" content="http://example.com/Pasted_image_20250305213648.png">
<meta property="og:image" content="http://example.com/Pasted_image_20250305214336.png">
<meta property="article:published_time" content="2025-05-30T06:05:23.177Z">
<meta property="article:modified_time" content="2025-05-30T15:37:58.119Z">
<meta property="article:author" content="采薇">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://example.com/1741338531957_d.png">
  
  
  
  <title>强化学习 - 采薇的博客</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 7.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>采薇的博客</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>生活Iteration</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>万物笔记Flow</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于采薇Kaelvio</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="强化学习"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-05-30 14:05" pubdate>
          2025年5月30日 下午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          2.3k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          20 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">强化学习</h1>
            
            
              <div class="markdown-body">
                
                <h1 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h1><p><img src="/1741338531957_d.png" srcset="/img/loading.gif" lazyload><br>Eg. Grid</p>
<p>state<br>state space S<br>action<br>action space A(s)<br>State transition</p>
<ul>
<li>forbidden area<br>Tabular representation: deterministic 因为事实上是三阶张量<br>State transition probability: all situation<br>Eg. p(s_2|s_1,a_2)</li>
</ul>
<p>Policy<br>π(a_1|s_1)<br>Tabular representation: fit for all<br>实现：随机采样</p>
<p>Reward in R(s,a)<br>Eg. r_bound&#x3D;-1,r_forbid&#x3D;-1,r_target&#x3D;1<br>table: deterministic<br>Mathematical: stochastic<br>Eg. p(r&#x3D;1|s1,a1)<br>Hardworking-&gt;positive<br>Trajectories轨迹: a state-action-reward chain<br>Return of .:sum of reward. use to compare policy<br>Discount rate γ:balance future ,prevent cycle收敛</p>
<p>Episode :usually finite, have terminal-&gt;episodic<br>Continuing:<br>Episodic-&gt; continuing: absorbing state or normal state with γ</p>
<p>MDP( Markov decision process )</p>
<ul>
<li>sets<ul>
<li>S</li>
<li>A(s)</li>
<li>R(s, a)</li>
</ul>
</li>
<li>Probability distribution <ul>
<li>State transition probability $p(v_f|v_i,a_k)$</li>
<li>Reward probability $p(r|v_f,a_k)$已考虑末状态和操作，不考虑初状态</li>
</ul>
</li>
<li>Policy<ul>
<li>π(a|s)也是一种概率</li>
</ul>
</li>
<li>Markov property与之前的状态无关<br>When decision is given -&gt;Markov process</li>
</ul>
<h1 id="Bellman-equation"><a href="#Bellman-equation" class="headerlink" title="Bellman equation"></a>Bellman equation</h1><p>State value: $v_\pi(s)&#x3D;\mathbb E[G_t|S_t&#x3D;s]$<br>Bootstrapping<br>矩阵形式$v_\pi&#x3D;r_\pi+\gamma P_\pi v_\pi$<br>展开形式<br>$$v_\pi(s)&#x3D;\sum_a \pi(a|s)\sum <em>r p(r|s,a) r+\gamma\sum_a \pi(a|s) \sum</em>{s’} p(s’|s,a)v_\pi(s’)&#x3D;\sum_a \pi(a|s)(\sum <em>r p(r|s,a) r+\gamma\sum</em>{s’} p(s’|s,a)v_\pi(s’))$$<br>$S_t\xrightarrow{A_t}R_{t+1},S_{t+1}$ random variables<br>G_t: return of certain trajectory<br>convergence：Cauchy</p>
<p>随机变量$X|Y,Z\rightarrow p^x_{yz}$，YZ为伪指标，缩并$value^{x}<em>{yz}$得到期望$\mathbb E</em>{yz}$<br>分上下指标或用<code>|</code>或<code>；</code>分隔，从右往左，相同则不缩并，不同则所并~numpy<br><img src="/IMG_0797.jpeg" srcset="/img/loading.gif" lazyload><br>求解Closed-form solution $v&#x3D;(1-\gamma P)^{-1}r$</p>
<ul>
<li><p>iterative solution 收敛</p>
</li>
<li><p>action value $q(s, a) :&#x3D; \mathbb{E} \left[ R_t \mid S_t &#x3D; s, A_t &#x3D; a \right]$<br>table: all situation<br>$$v_\pi (s)&#x3D;\sum_a \pi(a|s)q_\pi(s,a)$$<br>$$q_\pi(s,a)&#x3D;\sum_r p(r|s,a)r+\gamma \sum _{s’}p(s’|s,a)v_\pi(s’)$$<br>不只考虑当前策略的</p>
</li>
</ul>
<p>Bellman optimal equation (BOE)<br>$$v(s)&#x3D;\max_\pi(\sum_a \pi(a|s)\sum <em>r p(r|s,a) r+\gamma\sum_a \pi(a|s) \sum</em>{s’} p(s’|s,a)v_\pi(s’))&#x3D;\max_a \pi(a|s)(\sum <em>r p(r|s,a) r+\gamma\sum</em>{s’} p(s’|s,a)v_\pi(s’))$$<br>矩阵形式$v_\pi&#x3D;\max <em>\pi(r_\pi +\gamma P_\pi v)$<br>注意括号里面向量v，理论上每个分量是关于π的函数，这里是对π取最值，使<strong>每个</strong>分量最大。而不是求无穷范数，也并没有引入模长<br>计算时可能不知道v(s’)需要值迭代等方法<br>分量是$\pi^a_s c</em>{sa}$ 固定s，只需找a使$c_{sa}$最大</p>
<ul>
<li>how: value iteration<ul>
<li>policy update:先求最大π&#x3D;argmax … 即最大值对的自变量的值（arguments of the maximum）</li>
<li>value update:再求v（s）(*)</li>
</ul>
</li>
<li>Contraction mapping theorem$||f(v_1)-f(v_2)||_\infty\leq \gamma ||v_1-v_2||_\infty$<ul>
<li>existence</li>
<li>uniqueness</li>
<li>algorithm: exponential</li>
</ul>
</li>
<li>deterministic, greedy只选最大的<ul>
<li>γ 减小，短视</li>
<li>惩罚 增加<ul>
<li>r-&gt;ar+b不变</li>
</ul>
</li>
<li>其实越晚到达奖励越少，所以不会扰远路</li>
</ul>
</li>
</ul>
<h1 id="value-iteration-policy-iteration"><a href="#value-iteration-policy-iteration" class="headerlink" title="value iteration &amp;policy iteration"></a>value iteration &amp;policy iteration</h1><p>dynamic programming, Model Based RL</p>
<p>policy iteration</p>
<ul>
<li>policy evaluation$v_{\pi <em>k}&#x3D;r</em>{\pi <em>k} +\gamma P</em>{\pi <em>k} v</em>{\pi _k}$<ul>
<li>iterative solution(*)直到误差足够小</li>
</ul>
</li>
<li>policy improvement$v_{\pi <em>{k+1}}&#x3D;\text{argmax} <em>{\pi }(r</em>{\pi <em>k} +\gamma P</em>{\pi} v</em>{\pi _k})$<ul>
<li>$v_{\pi <em>{k+1}}$&gt;$v</em>{\pi _{k}}$</li>
<li>converges to an optimal policy</li>
</ul>
</li>
<li>接近目标的先训练好</li>
</ul>
<p>Truncated(截断) policy iteration algorithm<br>(*)步只进行j次<br><img src="/Pasted_image_20250305200350.png" srcset="/img/loading.gif" lazyload></p>
<p>GPI: generalized policy iteration</p>
<h1 id="Monto-Carlo-Learning"><a href="#Monto-Carlo-Learning" class="headerlink" title="Monto Carlo Learning"></a>Monto Carlo Learning</h1><blockquote>
<p>Without model, we need data!</p>
</blockquote>
<p>model-free没有p的值，依赖数据(experience)<br>mean estimation<br>Law of Large number: $\mathbb E &#x3D;\mathbb E [ x]$,$Var[\bar X]&#x3D;\frac 1N{Var}{[X]}$</p>
<h2 id="采样方法"><a href="#采样方法" class="headerlink" title="采样方法"></a>采样方法</h2><p>MC basic</p>
<ul>
<li>policy evaluation:采样估计q<ul>
<li>episode length</li>
</ul>
</li>
<li>policy improvement:</li>
</ul>
<p>MC Exploring Starts<br>visit::state-action pair<br><img src="/Pasted_image_20250305203222.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>first visit</li>
<li>very visit<br>从后往前<br>Exploring：遍历<br>Starts：从每个（s，a）开始</li>
</ul>
<p>MC ε-Greedy<br>ε&#x3D;0最优,逐渐减小<br> soft policy: stochastic<br>without Exploring Starts<br>$$<br>\pi(a|s) &#x3D; \begin{cases} 1 - \frac{\epsilon}{|\mathcal{A}|}({|\mathcal{A}|}-1) &amp; \text{if } a &#x3D; \text{argmax}_a Q(s, a) \ \frac{\epsilon}{|\mathcal{A}|} &amp; \text{if } a \neq \text{argmax}_a Q(s, a) \end{cases}<br>$$</p>
<ul>
<li>拿出ε均分，剩下的给greedy<br>exploitation&amp; exploration~[[练习|展示区&amp;学习区]]<br>π在大π-ε中找，every visit</li>
</ul>
<h1 id="Stochastic-Approximation"><a href="#Stochastic-Approximation" class="headerlink" title="Stochastic Approximation"></a>Stochastic Approximation</h1><p>incremental<br>eg. $\bar x$增量式估计$w_{k+1}&#x3D;w_{k}-a_k(w_{k}-x_{k})$    $a_k&#x3D;\frac 1n$<br>$\Delta x &#x3D; -ae(x)$~梯度下降，负反馈，a：步长</p>
<ul>
<li><input disabled="" type="checkbox"> <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/595113029">鞅论（Martingales）与随机逼近（Stochastic approximation）</a> 保证了随机过程的收敛性</li>
</ul>
<p>Robbins-Monro algorithm<br>$g(w)&#x3D;0$ 不知道g<br>$w_{k+1}&#x3D;w_k-a_k\tilde g(w_k,\eta _k)$   $\eta _k$噪声<br>$\lim w_k&#x3D;w^*$<br><img src="/Pasted_image_20250305221218.png" srcset="/img/loading.gif" lazyload></p>
<p><img src="/Pasted_image_20250305221357.png" srcset="/img/loading.gif" lazyload><br>w.p.1: with probability 1&#x3D;almost surely~测度论<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/92853437">XI.1 几乎处处 Almost Everywhere - Skye的文章 - 知乎</a></p>
<ol>
<li>递增,有界<ul>
<li>g&#x3D;∇L 找的是损失函数</li>
</ul>
</li>
<li>收敛于0,但不太快<ul>
<li>w_1不影响</li>
<li>eg. 1&#x2F;k, sufficiently small number</li>
</ul>
</li>
<li>eg. iid. independent and identically distributed</li>
</ol>
<p>In general<br><img src="/Pasted_image_20250305223543.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>允许$\beta$很快收敛到0</li>
<li>运用伪鞅：期望不变</li>
<li>证明RM：运用中值定理$\Delta g(w)&#x3D;\Delta w \cdot g’(w)$ </li>
<li>multiple variables:<img src="/1741332389063_d.png" srcset="/img/loading.gif" lazyload></li>
</ul>
<h2 id="Stochastic-gradient-descent"><a href="#Stochastic-gradient-descent" class="headerlink" title="Stochastic gradient descent"></a>Stochastic gradient descent</h2><p>eg. mean estimation<br>优化$\min_w J(w)&#x3D;\mathbb E[f(w,X)]$</p>
<p>gradient decent(GD)<br>let $\nabla_w \mathbb E[f(w,X)]&#x3D;0$<br>$$w_{k+1}&#x3D;w_k-a_k\nabla_w \mathbb E[f(w_k,X)]&#x3D;w_k-a_k \mathbb E[\nabla_wf(w_k,X)]&#x3D;0$$</p>
<p>batch gradient decent(BGD)<br>$\mathbb E[\nabla_wf(w,X)]\approx \frac 1n…$</p>
<ul>
<li>函数需要在模型训练之前定义</li>
</ul>
<p>Stochastic gradient decent(SGD)<br>$w_{k+1}&#x3D;w_k-a_k\nabla_w f(w_k,x_k)$<br>n&#x3D;1<br>convergence</p>
<ul>
<li>SGD is a special RM algorithm </li>
<li>较远时收敛较快<br><img src="/Pasted_image_20250305230829.png" srcset="/img/loading.gif" lazyload><br>deterministic-&gt;随机抽取</li>
</ul>
<p>mini batch gradient decent(MBGD)<br>m&#x3D;n时由于随机抽取，任有可能不是全部的</p>
<h1 id="Temporal-Difference-Learning"><a href="#Temporal-Difference-Learning" class="headerlink" title="Temporal-Difference Learning"></a>Temporal-Difference Learning</h1><p>incremental, online, bootstrapping, low estimation variance, bias&lt;-&gt;MC无偏估计<br>$$q_{t+1}(s_t,a_t)&#x3D;q_{t}(s_t,a_t)-\alpha_t(s_t,a_t)[q_{t}(s_t,a_t)-(r_{t+1}+\gamma \bar q_t)]$$</p>
<p>$$\underbrace{v_{t+1}\left(s_{t}\right)}<em>{\text {new estimate }}&#x3D;\underbrace{v</em>{t}\left(s_{t}\right)}<em>{\text {current estimate }}-\alpha</em>{t}\left(s_{t}\right)[\overbrace{v_{t}\left(s_{t}\right)-(\underbrace{r_{t+1}+\gamma v_{t}\left(s_{t+1}\right)}<em>{\text {TD target } \bar{v}</em>{t}})}^{\text {TD error } \delta_{t}}]$$<br>change form: $v_{t+1}\left(s_{t}\right)-\bar{v}<em>{t}&#x3D;\left[1-\alpha</em>{t}\left(s_{t}\right)\right]\left[v_{t}\left(s_{t}\right)-\bar{v}_{t}\right]$, so it is called Temporal-Difference </p>
<p>Bellman expectation equation<br>RM算法中其实函数是未知的，用$v_k(s_k’)$代替$v_\pi(s_k’)$<br>收敛性：由于$v_t(s)$收敛于最大值，故$v_{t+1}(s)-v_t(s)$收敛于0且（sup，理解为带有噪声的）单调</p>
<h2 id="Sarsa"><a href="#Sarsa" class="headerlink" title="Sarsa"></a>Sarsa</h2><p>Sarsa:$(s_t,a_t,r_{t+1},s_{t+1},a_{t+1})$<br>$$q_{t+1}(s_t,a_t)&#x3D;q_{t}(s_t,a_t)-\alpha_t(s_t,a_t)[q_{t}(s_t,a_t)-(r_{t+1}+\gamma q_t(s_{t+1},a_{t+1}))]$$</p>
<ul>
<li>这里的a_t+1是根据policy选的</li>
<li>policy evaluation</li>
<li>policy update<ul>
<li>ε-greedy</li>
</ul>
</li>
<li>特定目标可以只训练部分visit</li>
</ul>
<p>Expected Sarsa<br>$$q_{t+1}(s_t,a_t)&#x3D;q_{t}(s_t,a_t)-\alpha_t(s_t,a_t)[q_{t}(s_t,a_t)-(r_{t+1}+\gamma \mathbb E [q_t(s_{t+1},A)])]$$</p>
<ul>
<li>综合考虑所有a_t+1</li>
</ul>
<p>n-step Sarsa<br>$$q_{t+1}(s_t,a_t)&#x3D;q_{t}(s_t,a_t)-\alpha_t(s_t,a_t)[q_{t}(s_t,a_t)-\sum_{i&#x3D;1}^nr ^{i-1}+\gamma^nq_t(s_{t+n},a_{t+n})]$$</p>
<ul>
<li>综合MC</li>
<li>综合variance &amp;bias</li>
</ul>
<h2 id="Q-learning"><a href="#Q-learning" class="headerlink" title="Q-learning"></a>Q-learning</h2><p>use $\pi <em>b$ (behavior policy)to generate episode${s_0,a_0,r_1,s_1,a_1…}$<br>for each step:<br>update q-value<br>$$q</em>{t+1}(s_t,a_t)&#x3D;q_{t}(s_t,a_t)-\alpha_t(s_t,a_t)[q_{t}(s_t,a_t)-(r_{t+1}+\gamma \max <em>{a \in A }q_t(s</em>{t+1},a))]$$<br>update policy $\pi_{T,t+1}$</p>
<ul>
<li>estimate optimal q</li>
<li>update do not need π: off-policy<ul>
<li>behavior policy</li>
<li>target policy</li>
<li>on-policy: behavior policy&#x3D;target policy</li>
<li>off-policy: behavior policy, target policy can be difference ~ exploitation&amp; exploration</li>
</ul>
</li>
<li>do not need ε-greedy: $\pi_b$ already have exploration</li>
</ul>
<h1 id="function-approximation"><a href="#function-approximation" class="headerlink" title="function approximation"></a>function approximation</h1><h1 id="v"><a href="#v" class="headerlink" title="v"></a>v</h1><p>input: table(discrete) -&gt;function(continues)</p>
<ul>
<li>storage</li>
<li>generalization ability</li>
</ul>
<p>estimate:$\hat v (s,w)&#x3D;\phi ^T(s)w$</p>
<ul>
<li>w: parameter vector<ul>
<li>We can add dimension to increase accuracy</li>
<li>we did not use it before because the correspondence rule before is natural</li>
</ul>
</li>
<li>$\phi (s)$: feature vector of s. </li>
<li>train to change w</li>
<li>linear function was widely used before. then $\phi (s)$ can be polynomial basis, Fourier basis.<ul>
<li>difficult to select</li>
<li>easy to analysis</li>
<li>can uniform tabular representation $\phi (s_i)&#x3D;e_i,\nabla _w\hat v(s_i,w)&#x3D;e_i$</li>
</ul>
</li>
<li>neural network are widely used nowadays</li>
</ul>
<p>objective function(true value  error)<br>$J(w)&#x3D;\mathbb E[v_\pi(S)-\hat v (S,w)]^2$</p>
<ul>
<li><p>goal: minimize J(w)</p>
</li>
<li><p>probability distribution of S</p>
<ul>
<li>uniform, but some states are more important</li>
<li>stationary, long-run behavior $d_\pi(s)$ <ul>
<li>$d_\pi^T&#x3D;d_\pi^T P_\pi$ eigenvalue</li>
</ul>
</li>
</ul>
</li>
<li><p>Bellman error</p>
</li>
<li><p>Projected Bellman error: 投到span$\phi (s)$</p>
</li>
</ul>
<p>Sarsa:<br>$$w_{t+1}&#x3D;w_{t}+\alpha_{t}\left[r_{t+1}+\gamma \hat{q}\left(s_{t+1}, a_{t+1}, w_{t}\right)-\hat{q}\left(s_{t}, a_{t}, w_{t}\right)\right] \nabla_{w} \hat{q}\left(s_{t}, a_{t}, w_{t}\right)$$<br>与table比多了$\nabla_{w} \hat{q}\left(s_{t}, a_{t}, w_{t}\right)$<br>Q-learning<br>$$w_{t+1}&#x3D;w_{t}+\alpha_{t}\left[r_{t+1}+\gamma \max <em>{a\in A} \hat{q}\left(s</em>{t+1}, a, w_{t}\right)-\hat{q}\left(s_{t}, a_{t}, w_{t}\right)\right] \nabla_{w} \hat{q}\left(s_{t}, a_{t}, w_{t}\right)$$</p>
<h2 id="Deep-Q-learning-DQN"><a href="#Deep-Q-learning-DQN" class="headerlink" title="Deep Q-learning(DQN)"></a>Deep Q-learning(DQN)</h2><p>network: function</p>
<ul>
<li>main network</li>
<li>target network: fixed for some time and copy main<br>$$J&#x3D;\mathbb{E}\left[\left(R+\gamma \max <em>{a \in \mathcal{A}\left(S^{\prime}\right)} \hat{q}\left(S^{\prime}, a, w</em>{T}\right)-\hat{q}(S, A, w)\right)^{2}\right]$$</li>
</ul>
<p>experience replay<br>replay buffer（缓冲区）$\mathcal{B} :&#x3D;\left{\left(s, a, r, s^{\prime}\right)\right}$<br>use a mini-batch randomly from B<br>(S,A)is seemed as index, which assume to be i.i.d. , and we can use it for times, so we collect it and replay. In tabular cases, (S,A) is naturally independent.<br>$s\to W\to\hat q(s,a_i)$</p>
<h1 id="π"><a href="#π" class="headerlink" title="π"></a>π</h1><p>Since s has been continuous, pi must use function to approach.<br>generate policy greedily from value -&gt;train policy, use metrics(度规) to judge policy. </p>
<p>average state value：<br>$$\bar{v}<em>{\pi}:&#x3D;\mathbb{E}\left[v</em>{\pi}(S)\right]&#x3D;d_sv^s_\pi&#x3D;\mathbb{E}\left[\sum_{t&#x3D;0}^{\infty} \gamma^{t} R_{t+1}\right]$$</p>
<ul>
<li>d independent to pi, denote as $d_0,\bar v_\pi^0$<ul>
<li>when interested in v_i, let d&#x3D;e_i</li>
</ul>
</li>
<li>d dependent on pi: stationary</li>
</ul>
<p>average reward<br>$$\bar{r}<em>{\pi} :&#x3D; \sum</em>{s \in \mathcal{S}} d_{\pi}(s) r_{\pi}(s)&#x3D;\mathbb{E}\left[r_{\pi}(S)\right]&#x3D;\lim <em>{n \rightarrow \infty} \frac{1}{n} \mathbb{E}\left[\sum</em>{k&#x3D;1}^{n} R_{t+k}\right]$$</p>
<ul>
<li>can be undiscounted</li>
<li>$\bar v_\pi&#x3D;\bar r_\pi +\gamma \bar v_\pi$</li>
</ul>
<p>$$\nabla_{\theta} J(\theta)&#x3D;\sum_{s \in \mathcal{S}} \eta(s) \sum_{a \in \mathcal{A}} \nabla_{\theta} \pi(a \mid s, \theta) q_{\pi}(s, a)$$<br>use dx&#x3D;x ln x , we get$&#x3D;\mathbb{E}\left[\nabla_{\theta} \ln \pi(A \mid S, \theta) q_{\pi}(S, A)\right]$<br>to fit  ln, we use softmax:<br>$$\pi(a \mid s, \theta)&#x3D;\frac{e^{\phi(s, a, \theta)}}{\sum_{a^{\prime} \in \mathcal{A}} e^{\phi\left(s, a^{\prime}, \theta\right)}}$$<br> a neural network can help you do this <strong>automatically</strong></p>
<ul>
<li>stochastic &amp; exploratory</li>
</ul>
<p>since $q_\pi$ is unknown<br>we can use MC (REINFORCE)<br>on policy<br>change form:  $\theta_{t+1}&#x3D;\theta_{t}+\alpha \underbrace{\left(\frac{q_{t}\left(s_{t}, a_{t}\right)}{\pi\left(a_{t} \mid s_{t}, \theta_{t}\right)}\right)}<em>{\beta</em>{t}} \nabla_{\theta} \pi\left(a_{t} \mid s_{t}, \theta_{t}\right)$ , which maximized (polarize, because the optimistic policy is deterministic) pi, balance exploitation &amp; exploration</p>
<h1 id="Actor-Critic"><a href="#Actor-Critic" class="headerlink" title="Actor-Critic"></a>Actor-Critic</h1><p>actor: update policy to act<br>critic: estimate policy to value </p>
<h2 id="QAC"><a href="#QAC" class="headerlink" title="QAC"></a>QAC</h2><p>use TD to estimate $q_\pi$<br>(calculate q-error and update q_t)</p>
<h2 id="A2C-advanced-AC"><a href="#A2C-advanced-AC" class="headerlink" title="A2C(advanced AC)"></a>A2C(advanced AC)</h2><p>baseline<br>$$b^{*}(s)&#x3D;\frac{\mathbb{E}<em>{A \sim \pi}\left[\left|\nabla</em>{\theta} \ln \pi\left(A \mid s, \theta_{t}\right)\right|^{2} q(s, A)\right]}{\mathbb{E}<em>{A \sim \pi}\left[\left|\nabla</em>{\theta} \ln \pi\left(A \mid s, \theta_{t}\right)\right|^{2}\right]}\approx v_\pi (s)$$</p>
<ul>
<li>does not change E, change Var<br>advanced function(Zero-centered): $\delta_{\pi} :&#x3D; q_{\pi}(S, A)-v_{\pi}(S)\approx r_{t+1}+\gamma v_t(s_{t+1})-v_t(s_t)$, (have the same expectation ) which is TD error<br>then we use delta to update both w for v(critic) &amp; θ for π(actor)</li>
</ul>
<h2 id="Off-policy-AC"><a href="#Off-policy-AC" class="headerlink" title="Off-policy AC"></a>Off-policy AC</h2><p>Off-policy</p>
<ul>
<li>之前的delta 依赖于pi</li>
<li>importance sample: 用分布p_1估计p_0</li>
<li>importance weight: $\frac {p_0(x_i)}{p_1(x_i)}$ </li>
<li>In continuous case, we can know $p_0(x_i)$, but we don not know $\mathbb E(X)&#x3D;\int p_0(x) dx$ </li>
<li>value update can be off-policy as before : 不涉及状态的更新</li>
<li>$\beta$ can be $\mu$+noise</li>
</ul>
<h2 id="Deterministic-AC-DAC"><a href="#Deterministic-AC-DAC" class="headerlink" title="Deterministic AC (DAC)"></a>Deterministic AC (DAC)</h2><p>之前用了soft max，不是真实的连续输出<br>实际上输出的也只是每个值的概率<br>$a^._s&#x3D;\mu (s;\theta)$ is deterministic ,实际上代替了$\pi^a_s$,本质上都是输入s输出a</p>
<p>$$J(\theta)&#x3D;\mathbb{E}\left[v_{\mu}(s)\right]&#x3D;\sum_{s \in \mathcal{S}} d(s) v_{\mu}(s)$$</p>
<ul>
<li>d independent to pi, denote as $d_0$<ul>
<li>when interested in v_i, let d&#x3D;e_i<br>$$\nabla_{\theta} J(\theta)&#x3D;\sum_{s \in \mathcal{S}} \rho_{\mu}(s) \nabla_{\theta} \mu(s)\left(\nabla_{\mu (s)} q_{\mu}(s, \mu(s))\right)$$</li>
</ul>
</li>
</ul>
<h1 id="MARL"><a href="#MARL" class="headerlink" title="MARL"></a>MARL</h1><p>Multi-Agent Reinforcement Learning</p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Uf4y1D7mx/?share_source=copy_web&vd_source=55f83207e6b9bcb77f93e22aac3e082a">【多智能体强化学习(1-2)：基本概念  Multi-Agent Reinforcement Learning】</a></p>
<ul>
<li>fully cooperative setting</li>
<li>fully competitive setting</li>
<li>Mixed cooperative &amp; cooperative</li>
<li>Self -Interested</li>
</ul>
<p>$A^i_t,R^i_t,S_t,U^i_t ~~return,\pi (a^i|s;\theta ^i)$<br>$V_i(s_t;\theta ^1…\theta ^n)&#x3D;\mathbb E[U^i_t|S_t&#x3D;s_t]$ depend on all theta</p>
<p>convergence收敛:无法让收益更高<br>Nash equilibrium: While all the other agents’ policy remain the same, certain agent remains the same.<br>互相影响</p>
<ul>
<li><p>partial observation: $o^i$ 第i个的观测</p>
</li>
<li><p>full observation</p>
</li>
<li><p>fully decentralized</p>
<ul>
<li>不收敛</li>
</ul>
</li>
<li><p>fully centralized:</p>
<ul>
<li>Actor-Critic method: $\pi(a^i|\mathbf o;\theta^i),q(\mathbf o,\mathbf a;w^i)$</li>
<li>slow</li>
</ul>
</li>
<li><p>Centralized training with decentralized execution</p>
<ul>
<li>$\pi(a^i| o^i;\theta^i),q(\mathbf o,\mathbf a;w^i)$<br><img src="/Pasted_image_20250305213648.png" srcset="/img/loading.gif" lazyload><br><img src="/Pasted_image_20250305214336.png" srcset="/img/loading.gif" lazyload><br>Parameter Sharing: 功能相同</li>
</ul>
</li>
</ul>
<h1 id="HRL"><a href="#HRL" class="headerlink" title="HRL"></a>HRL</h1><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/tensorflow/models/tree/master/research/efficient-hrl">tensorflow的研究</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/rcsoccersim/rcssserver">一个足球模拟11v11但是c++</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/metadriverse/metadrive">自动驾驶相关</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/ray-project/ray/blob/master/rllib/examples/hierarchical/hierarchical_training.py">https://github.com/ray-project/ray/blob/master/rllib/examples/hierarchical/hierarchical_training.py</a><br>但我发现很多地方看不懂，我可能需要先补一下前面的内容</li>
</ul>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>强化学习</div>
      <div>http://example.com/2025/05/30/Blog/强化学习/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>采薇</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年5月30日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/05/30/Blog/%E4%BA%9A%E6%96%87%E5%8C%96(1)/" title="亚文化(1)">
                        <span class="hidden-mobile">亚文化(1)</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
